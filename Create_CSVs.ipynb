{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from metadata generated by H20 and loading it into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrir\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>balance</th>\n",
       "      <th>balance_threshold</th>\n",
       "      <th>classification</th>\n",
       "      <th>data_path</th>\n",
       "      <th>end_time</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>file_creation_time</th>\n",
       "      <th>max_models</th>\n",
       "      <th>min_mem_size</th>\n",
       "      <th>...</th>\n",
       "      <th>nthreads</th>\n",
       "      <th>project</th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_path</th>\n",
       "      <th>run_time</th>\n",
       "      <th>scale</th>\n",
       "      <th>server_path</th>\n",
       "      <th>start_time</th>\n",
       "      <th>target</th>\n",
       "      <th>test_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.555891e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-26 16:09:35</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12/58erhH7N2</td>\n",
       "      <td>900</td>\n",
       "      <td>False</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12</td>\n",
       "      <td>1.555891e+09</td>\n",
       "      <td>T_degC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.555892e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-26 16:09:35</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>lOBYWUG72</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12/58erhH7N2/lOBYWUG72</td>\n",
       "      <td>1100</td>\n",
       "      <td>False</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12/58erhH7N2</td>\n",
       "      <td>1.555892e+09</td>\n",
       "      <td>T_degC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.555890e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-26 16:09:35</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>p7TZ8WufJ</td>\n",
       "      <td>/content/p7TZ8WufJ</td>\n",
       "      <td>500</td>\n",
       "      <td>False</td>\n",
       "      <td>/content</td>\n",
       "      <td>1.555890e+09</td>\n",
       "      <td>T_degC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.555890e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-26 16:09:35</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>qlXRazR12</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12</td>\n",
       "      <td>700</td>\n",
       "      <td>False</td>\n",
       "      <td>/content/p7TZ8WufJ</td>\n",
       "      <td>1.555890e+09</td>\n",
       "      <td>T_degC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.555894e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-04-26 16:09:35</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>xmbfawAR9</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12/58erhH7N2/lOBYWUG...</td>\n",
       "      <td>1300</td>\n",
       "      <td>False</td>\n",
       "      <td>/content/p7TZ8WufJ/qlXRazR12/58erhH7N2/lOBYWUG72</td>\n",
       "      <td>1.555894e+09</td>\n",
       "      <td>T_degC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  balance  balance_threshold  classification data_path  \\\n",
       "0         0    False                0.2           False      None   \n",
       "0         0    False                0.2           False      None   \n",
       "0         0    False                0.2           False      None   \n",
       "0         0    False                0.2           False      None   \n",
       "0         0    False                0.2           False      None   \n",
       "\n",
       "       end_time  execution_time   file_creation_time  max_models  \\\n",
       "0  1.555891e+09             0.0  2019-04-26 16:09:35           9   \n",
       "0  1.555892e+09             0.0  2019-04-26 16:09:35           9   \n",
       "0  1.555890e+09             0.0  2019-04-26 16:09:35           9   \n",
       "0  1.555890e+09             0.0  2019-04-26 16:09:35           9   \n",
       "0  1.555894e+09             0.0  2019-04-26 16:09:35           9   \n",
       "\n",
       "   min_mem_size    ...    nthreads  project     run_id  \\\n",
       "0             6    ...           1     None  58erhH7N2   \n",
       "0             6    ...           1     None  lOBYWUG72   \n",
       "0             6    ...           1     None  p7TZ8WufJ   \n",
       "0             6    ...           1     None  qlXRazR12   \n",
       "0             6    ...           1     None  xmbfawAR9   \n",
       "\n",
       "                                            run_path run_time  scale  \\\n",
       "0             /content/p7TZ8WufJ/qlXRazR12/58erhH7N2      900  False   \n",
       "0   /content/p7TZ8WufJ/qlXRazR12/58erhH7N2/lOBYWUG72     1100  False   \n",
       "0                                 /content/p7TZ8WufJ      500  False   \n",
       "0                       /content/p7TZ8WufJ/qlXRazR12      700  False   \n",
       "0  /content/p7TZ8WufJ/qlXRazR12/58erhH7N2/lOBYWUG...     1300  False   \n",
       "\n",
       "                                        server_path    start_time  target  \\\n",
       "0                      /content/p7TZ8WufJ/qlXRazR12  1.555891e+09  T_degC   \n",
       "0            /content/p7TZ8WufJ/qlXRazR12/58erhH7N2  1.555892e+09  T_degC   \n",
       "0                                          /content  1.555890e+09  T_degC   \n",
       "0                                /content/p7TZ8WufJ  1.555890e+09  T_degC   \n",
       "0  /content/p7TZ8WufJ/qlXRazR12/58erhH7N2/lOBYWUG72  1.555894e+09  T_degC   \n",
       "\n",
       "  test_path  \n",
       "0      None  \n",
       "0      None  \n",
       "0      None  \n",
       "0      None  \n",
       "0      None  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import os to walk through the directories and open the files and get their creation time\n",
    "import os\n",
    "#import json to read the json metadata files and the hyperparameter files\n",
    "import json\n",
    "#import pandas to store the data scraped from the JSON files\n",
    "import pandas as pd\n",
    "#import time to format the creation time of the files\n",
    "import time\n",
    "#import json_normalize to flatten the json files before storing it in a dataframe\n",
    "from pandas.io.json import json_normalize\n",
    "#create and empty dataframe called runs\n",
    "runs = pd.DataFrame()\n",
    "#set the root directory as the folder containing the run outputs generated by H20\n",
    "rootdir ='Extracted Files'\n",
    "#Loop through the folders and sub-folders in the \"Extracted Files\" folder\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    #loop through the files\n",
    "    for file in files:\n",
    "        #we are interested only in the metadata json files in this section of the code\n",
    "        if (file.endswith('json') and file.startswith('meta')):\n",
    "            fullfilepath = subdir+\"/\"+file\n",
    "            #open the file\n",
    "            with open(str(subdir)+\"/\"+file) as f:\n",
    "                #load the flattened JSON file into a dictionary\n",
    "                d = json.load(f)                \n",
    "                #flatten the dictionary using json_normalize() and append the output to the pandas dataframe that we created earlier called runs\n",
    "                runs = df.append(json_normalize(d))\n",
    "                #change the format of the file creation time into a format acceptable by the database\n",
    "                runs['file_creation_time']=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getctime(subdir+\"/\"+file)))\n",
    "            #close the file    \n",
    "            f.close()\n",
    "#Export the data frame to csv which we will later directly import into a stage table in our MySQL database\n",
    "runs.to_csv(\"Runs.csv\",index=False)\n",
    "runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Hyperparameter Csv file from Hyperparameter Json\n",
    "\n",
    "In the code below we loop through hyperparameter list to find their values in the Json files and creating a dataframe out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Id</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Hyperparameter_Name</th>\n",
       "      <th>Default_Value</th>\n",
       "      <th>Actual_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_233433_model_1</td>\n",
       "      <td>fold_assignment</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>Modulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_233433_model_1</td>\n",
       "      <td>max_runtime_secs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_233433_model_1</td>\n",
       "      <td>balance_classes</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_233433_model_1</td>\n",
       "      <td>max_after_balance_size</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_233433_model_1</td>\n",
       "      <td>seed</td>\n",
       "      <td>-1</td>\n",
       "      <td>623814104656333214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Run_Id                                 Model_Name  \\\n",
       "0  58erhH7N2  GLM_grid_1_AutoML_20190421_233433_model_1   \n",
       "1  58erhH7N2  GLM_grid_1_AutoML_20190421_233433_model_1   \n",
       "2  58erhH7N2  GLM_grid_1_AutoML_20190421_233433_model_1   \n",
       "3  58erhH7N2  GLM_grid_1_AutoML_20190421_233433_model_1   \n",
       "4  58erhH7N2  GLM_grid_1_AutoML_20190421_233433_model_1   \n",
       "\n",
       "      Hyperparameter_Name Default_Value        Actual_Value  \n",
       "0         fold_assignment          AUTO              Modulo  \n",
       "1        max_runtime_secs             0                   0  \n",
       "2         balance_classes         False               False  \n",
       "3  max_after_balance_size             5                   5  \n",
       "4                    seed            -1  623814104656333214  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "#loading the list of hyperparameters into a dictionary\n",
    "with open('Hyperparameters.txt', \"r\") as word_list:\n",
    "    hyperparams = word_list.read().split(' ')\n",
    "hyperparams = hyperparams[0].split(\"\\n\")\n",
    "\n",
    "hyperparams = list(dict.fromkeys(hyperparams)) #loading into a list to Remove duplicates\n",
    "\n",
    "#Making a list of files of Json files \n",
    "hyperparam_json_files=['Extracted Files/58erhH7N2/hyperparameters_900 (1).json',\n",
    "                      'Extracted Files/lOBYWUG72/hyperparameters_1100 (1).json',\n",
    "                      'Extracted Files/p7TZ8WufJ/hyperparameters_500.json',\n",
    "                      'Extracted Files/qlXRazR12/hyperparameters_700 (1).json',\n",
    "                      'Extracted Files/xmbfawAR9/hyperparameters_1300 (1).json']\n",
    "\n",
    "\n",
    "#Preparing lists to load information from the json file\n",
    "m=[]\n",
    "h=[]\n",
    "d=[]\n",
    "a=[]\n",
    "id=[]\n",
    "\n",
    "#loop to iterate through the hyperparameter json files\n",
    "for file in hyperparam_json_files:\n",
    "#loading the Json\n",
    "    with open(file, 'r') as f:\n",
    "        distros_dict = json.load(f)\n",
    "\n",
    "#Extracting the data from Json to fit the model \n",
    "\n",
    "    i = 0 #Setting the counter\n",
    "    for item in distros_dict: #looping through every item in the json file\n",
    "        for hyper in hyperparams: #looping through necessary hyperparamter in the text file\n",
    "            \n",
    "            id.append(os.path.dirname(file).split('/')[-1])#getting the run id\n",
    "            m.append(distros_dict[i]['model_id']['actual']['name'])#name of the model\n",
    "            h.append(hyper)#getting the hyperparameter\n",
    "            try:\n",
    "                d.append(distros_dict[i][hyper]['default'])#getting the default value\n",
    "            except:\n",
    "                d.append(None)\n",
    "            try:\n",
    "                a.append(distros_dict[i][hyper]['actual'])#getting the actual value\n",
    "            except:\n",
    "                a.append(None)\n",
    "                \n",
    "        i+=1\n",
    "\n",
    "#Creating a dataframe to store the data\n",
    "\n",
    "dataframe = pd.DataFrame({'Run_Id':id,\n",
    "                          'Model_Name' : m,\n",
    "                          'Hyperparameter_Name': h,\n",
    "                          'Default_Value' : d,\n",
    "                          'Actual_Value' : a})\n",
    "\n",
    "#dropping records where the hyperparamter is absent for the algorithm\n",
    "\n",
    "dataframe = dataframe.dropna().reset_index(drop=True)\n",
    "\n",
    "#Converting to csv\n",
    "dataframe.to_csv('Hyperparameter.csv',index=False)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model Csv file from Leaderboard Csv\n",
    "\n",
    "In the following code, we loop through the leaderboard csv and create a dataframe consisting of Run Ids, Model Names and their Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>mean_residual_deviance</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>rmsle</th>\n",
       "      <th>auc</th>\n",
       "      <th>log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_233433_model_1</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.042093</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_234613_model_1</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.042093</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>GLM_grid_1_AutoML_20190421_232600_model_1</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.042093</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>StackedEnsemble_BestOfFamily_AutoML_20190421_2...</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.051266</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.017612</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58erhH7N2</td>\n",
       "      <td>StackedEnsemble_AllModels_AutoML_20190421_232600</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.010259</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      run_id                                           model_id  \\\n",
       "0  58erhH7N2          GLM_grid_1_AutoML_20190421_233433_model_1   \n",
       "1  58erhH7N2          GLM_grid_1_AutoML_20190421_234613_model_1   \n",
       "2  58erhH7N2          GLM_grid_1_AutoML_20190421_232600_model_1   \n",
       "3  58erhH7N2  StackedEnsemble_BestOfFamily_AutoML_20190421_2...   \n",
       "4  58erhH7N2   StackedEnsemble_AllModels_AutoML_20190421_232600   \n",
       "\n",
       "   mean_residual_deviance      rmse       mse       mae     rmsle   auc  \\\n",
       "0                0.001772  0.042093  0.001772  0.016516  0.005909  None   \n",
       "1                0.001772  0.042093  0.001772  0.016516  0.005909  None   \n",
       "2                0.001772  0.042093  0.001772  0.016516  0.005909  None   \n",
       "3                0.002628  0.051266  0.002628  0.017612  0.005495  None   \n",
       "4                0.002952  0.054336  0.002952  0.010259  0.005552  None   \n",
       "\n",
       "  log_loss  \n",
       "0     None  \n",
       "1     None  \n",
       "2     None  \n",
       "3     None  \n",
       "4     None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "#Creating a list to load the files\n",
    "files=[]\n",
    "\n",
    "#For everyfile ending with .csv load it into 'files'\n",
    "for filename in os.listdir('Leaderboard CSVs'):\n",
    "    if filename.endswith(\".csv\"):# or filename.endswith(\".py\"): \n",
    "        files.append(os.path.join('Leaderboard CSVs', filename))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#looping through the files to get the new dataframe with individual run-ids\n",
    "\n",
    "run_id = []\n",
    "i=0\n",
    "new_df= pd.DataFrame()\n",
    "for file in files:\n",
    "    id = os.path.basename(file)\n",
    "    run_id.append(id.split('_')[0].split('lb')[0])\n",
    "    df=pd.read_csv(file,index_col=0)\n",
    "    df.insert(0, 'run_id', run_id[i])\n",
    "    new_df = new_df.append(df)\n",
    "    i=i+1\n",
    "\n",
    "#It is Null since our dataset is a regression one\n",
    "new_df['auc'] = None\n",
    "new_df['log_loss'] = None\n",
    "\n",
    "new_df.to_csv('Leaderboard.csv',index=False)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tags from the dataset description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>tag_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biological</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CalCOFI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zooplankton</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>world</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tag  tag_frequency\n",
       "0   California              6\n",
       "1   biological              4\n",
       "2      CalCOFI              3\n",
       "3  zooplankton              2\n",
       "4        world              2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. The physical, chemical, and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the California Current and a range of biological responses to them. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature.The California Cooperative Oceanic Fisheries Investigations (CalCOFI) are a unique partnership of the California Department of Fish & Wildlife, NOAA Fisheries Service and Scripps Institution of Oceanography. The organization was formed in 1949 to study the ecological aspects of the sardine population collapse off California. Today our focus has shifted to the study of the marine environment off the coast of California, the management of its living resources, and monitoring the indicators of El Nino and climate change. CalCOFI conducts quarterly cruises off southern & central California, collecting a suite of hydrographic and biological data on station and underway. Data collected at depths down to 500 m include: temperature, salinity, oxygen, phosphate, silicate, nitrate and nitrite, chlorophyll, transmissometer, PAR, C14 primary productivity, phytoplankton biodiversity, zooplankton biomass, and zooplankton biodiversity.\"\n",
    "BAD_CHARS = \".!?,\\'\\\"\"\n",
    "\n",
    "# transform text into a list words--removing punctuation and filtering small words\n",
    "words = [ word.strip(BAD_CHARS) for word in text.strip().split() if len(word) > 4 ]\n",
    "\n",
    "word_freq = {}\n",
    "\n",
    "# generate a 'word histogram' for the text--ie, a list of the frequencies of each word\n",
    "for word in words :\n",
    "  word_freq[word] = word_freq.get(word, 0) + 1\n",
    "  \n",
    "type(word_freq)\n",
    "\n",
    "\n",
    "# sort the word list by frequency \n",
    "# (just a DSU sort, there's a python built-in for this, but i can't remember it)\n",
    "tx = [ (v, k) for (k, v) in word_freq.items()]\n",
    "tx.sort(reverse=True)\n",
    "word_freq_sorted = [ (k, v) for (v, k) in tx ]\n",
    "\n",
    "# eg, what are the most common words in that text?\n",
    "tags = pd.DataFrame(word_freq_sorted)\n",
    "\n",
    "#rename the columns in the tags dataframe as \"tags\" and \"tag_frequency\"\n",
    "tags.columns = ['tag', 'tag_frequency']\n",
    "#remove tags with numbers for the sake of simplicity when importing the data into MySQL\n",
    "#tags = df[~df.tag.str.contains(r'\\d')]\n",
    "#Export the tags dataframe to csv to import it later into MySQL\n",
    "tags.to_csv('tags.csv',index=False)\n",
    "\n",
    "tags.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
